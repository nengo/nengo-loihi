{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyword spotting\n",
    "\n",
    "In the keyword spotting task,\n",
    "the goal is to identify a target phrase\n",
    "in an audio speech signal.\n",
    "In this example, we solve the keyword spotting task\n",
    "in a deep neural network trained\n",
    "with [NengoDL](https://www.nengo.ai/nengo-dl/)\n",
    "and converted to a normal Nengo model\n",
    "that we run on NengoLoihi.\n",
    "\n",
    "This example uses optimized parameters\n",
    "generated by NengoDL:\n",
    "\n",
    "* [reference_params.pkl](\n",
    "https://drive.google.com/open?id=149rLqXnJqZPBiqvWpOAysGyq4fvunlnM)\n",
    "* [test_stream.pkl](\n",
    "https://drive.google.com/open?id=1AQavHjQKNu1sso0jqYhWj6zUBLKuGNvV)\n",
    "\n",
    "If you have `requests` installed,\n",
    "we will attempt to download these files automatically.\n",
    "If not, you can download the files manually and\n",
    "place them in the same directory as this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import string\n",
    "\n",
    "import nengo\n",
    "import numpy as np\n",
    "try:\n",
    "    import requests\n",
    "    has_requests = True\n",
    "except ImportError:\n",
    "    has_requests = False\n",
    "\n",
    "import nengo_loihi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These helper functions simplify the rest of the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(chars):\n",
    "    \"\"\"Merge repeated characters and strip blank CTC symbol\"\"\"\n",
    "    acc = [\"-\"]\n",
    "    for c in chars:\n",
    "        if c != acc[-1]:\n",
    "            acc.append(c)\n",
    "\n",
    "    acc = [c for c in acc if c != \"-\"]\n",
    "    return \"\".join(acc)\n",
    "\n",
    "\n",
    "class Linear(nengo.neurons.NeuronType):\n",
    "    probeable = (\"rates\",)\n",
    "\n",
    "    def __init__(self, amplitude=1):\n",
    "        super(Linear, self).__init__()\n",
    "\n",
    "        self.amplitude = amplitude\n",
    "\n",
    "    def gain_bias(self, max_rates, intercepts):\n",
    "        \"\"\"Determine gain and bias by shifting and scaling the lines.\"\"\"\n",
    "        max_rates = np.array(max_rates, dtype=float, copy=False, ndmin=1)\n",
    "        intercepts = np.array(intercepts, dtype=float, copy=False, ndmin=1)\n",
    "        gain = max_rates / (1 - intercepts)\n",
    "        bias = -intercepts * gain\n",
    "        return gain, bias\n",
    "\n",
    "    def max_rates_intercepts(self, gain, bias):\n",
    "        \"\"\"Compute the inverse of gain_bias.\"\"\"\n",
    "        intercepts = -bias / gain\n",
    "        max_rates = gain * (1 - intercepts)\n",
    "        return max_rates, intercepts\n",
    "\n",
    "    def step_math(self, dt, J, output):\n",
    "        \"\"\"Implement the rectification nonlinearity.\"\"\"\n",
    "        output[...] = self.amplitude * J\n",
    "\n",
    "\n",
    "def download(fname, drive_id):\n",
    "    \"\"\"Download a file from Google Drive.\n",
    "\n",
    "    Adapted from https://stackoverflow.com/a/39225039/1306923\n",
    "    \"\"\"\n",
    "    def get_confirm_token(response):\n",
    "        for key, value in response.cookies.items():\n",
    "            if key.startswith('download_warning'):\n",
    "                return value\n",
    "        return None\n",
    "\n",
    "    def save_response_content(response, destination):\n",
    "        CHUNK_SIZE = 32768\n",
    "\n",
    "        with open(destination, \"wb\") as f:\n",
    "            for chunk in response.iter_content(CHUNK_SIZE):\n",
    "                if chunk:  # filter out keep-alive new chunks\n",
    "                    f.write(chunk)\n",
    "\n",
    "    url = \"https://docs.google.com/uc?export=download\"\n",
    "    session = requests.Session()\n",
    "    response = session.get(url, params={'id': drive_id}, stream=True)\n",
    "    token = get_confirm_token(response)\n",
    "    if token is not None:\n",
    "        params = {'id': drive_id, 'confirm': token}\n",
    "        response = session.get(url, params=params, stream=True)\n",
    "    save_response_content(response, fname)\n",
    "\n",
    "\n",
    "def load(fname, drive_id):\n",
    "    if not os.path.exists(fname):\n",
    "        if has_requests:\n",
    "            print(\"Downloading %s...\" % fname)\n",
    "            download(fname, drive_id)\n",
    "            print(\"Saved %s to %s\" % (fname, os.getcwd()))\n",
    "        else:\n",
    "            link = \"https://drive.google.com/open?id=%s\" % drive_id\n",
    "            raise RuntimeError(\n",
    "                \"Cannot find '%s'. Download the file from\\n  %s\\n\"\n",
    "                \"and place it in %s.\" % (fname, link, os.getcwd()))\n",
    "    print(\"Loading %s\" % fname)\n",
    "    with open(fname, \"rb\") as fp:\n",
    "        ret = pickle.load(fp)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some parameters and load the data from files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 390\n",
    "n_outputs = 29\n",
    "n_neurons = 256\n",
    "\n",
    "allowed_text = [\"loha\", \"alha\", \"aloa\", \"aloh\", \"aoha\", \"aloha\"]\n",
    "id_to_char = np.array(list(string.ascii_lowercase + \"\\\" -|\"))\n",
    "\n",
    "params = load(\"reference_params.pkl\", \"149rLqXnJqZPBiqvWpOAysGyq4fvunlnM\")\n",
    "test_stream = load(\"test_stream.pkl\", \"1AQavHjQKNu1sso0jqYhWj6zUBLKuGNvV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# core speech model for keyword spotting\n",
    "with nengo.Network(label=\"Keyword spotting\") as model:\n",
    "    nengo_loihi.add_params(model)\n",
    "    model.config[nengo.Connection].synapse = None\n",
    "\n",
    "    # network was trained with amplitude of 0.002\n",
    "    # scaling up improves performance on Loihi\n",
    "    neuron_type = nengo.LIF(tau_rc=0.02,\n",
    "                            tau_ref=0.001,\n",
    "                            amplitude=0.005)\n",
    "\n",
    "    # below is the core model architecture\n",
    "    inp = nengo.Node(np.zeros(n_inputs), label=\"in\")\n",
    "\n",
    "    layer_1 = nengo.Ensemble(n_neurons=n_neurons, dimensions=1,\n",
    "                             neuron_type=neuron_type,\n",
    "                             gain=params[\"x_c_0\"][\"gain\"],\n",
    "                             bias=params[\"x_c_0\"][\"bias\"],\n",
    "                             label=\"Layer 1\")\n",
    "    model.config[layer_1].on_chip = False\n",
    "    nengo.Connection(\n",
    "        inp, layer_1.neurons, transform=params[\"input_node -> x_c_0\"])\n",
    "\n",
    "    layer_2 = nengo.Ensemble(n_neurons=n_neurons, dimensions=1,\n",
    "                             neuron_type=neuron_type,\n",
    "                             gain=params[\"x_c_1\"][\"gain\"],\n",
    "                             bias=params[\"x_c_1\"][\"bias\"],\n",
    "                             label=\"Layer 2\")\n",
    "    nengo.Connection(\n",
    "        layer_1.neurons, layer_2.neurons,\n",
    "        transform=params[\"x_c_0 -> x_c_1\"])\n",
    "\n",
    "    char_synapse = nengo.synapses.Alpha(0.005)\n",
    "\n",
    "    # --- char_out as node\n",
    "    char_out = nengo.Node(None, label=\"out\", size_in=n_outputs)\n",
    "    char_output_bias = nengo.Node(params[\"char_output_bias\"])\n",
    "    nengo.Connection(char_output_bias, char_out, synapse=None)\n",
    "    nengo.Connection(\n",
    "        layer_2.neurons, char_out,\n",
    "        transform=params[\"x_c_1 -> char_output\"])\n",
    "    char_probe = nengo.Probe(char_out, synapse=char_synapse)\n",
    "\n",
    "    model.inp = inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_text(sim, n_steps, p_time):\n",
    "    \"\"\"Predict a text transcription from the current simulation state\"\"\"\n",
    "    n_frames = int(n_steps / p_time)\n",
    "    char_data = sim.data[char_probe]\n",
    "    n_chars = char_data.shape[1]\n",
    "\n",
    "    # reshape to separate out each window frame that was presented\n",
    "    char_out = np.reshape(char_data, (n_frames, p_time, n_chars))\n",
    "\n",
    "    # take most ofter predicted char over each frame presentation interval\n",
    "    char_ids = np.argmax(char_out, axis=2)\n",
    "    char_ids = [np.argmax(np.bincount(i)) for i in char_ids]\n",
    "\n",
    "    text = merge(\"\".join([id_to_char[i] for i in char_ids]))\n",
    "    text = merge(text)  # merge repeats to help autocorrect\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = {\n",
    "    \"fp\": 0,\n",
    "    \"tp\": 0,\n",
    "    \"fn\": 0,\n",
    "    \"tn\": 0,\n",
    "    \"aloha\": 0,\n",
    "    \"not-aloha\": 0,\n",
    "}\n",
    "\n",
    "for arrays, text, speaker_id, _ in test_stream[:10]:\n",
    "    dt = 0.001\n",
    "    stream = arrays[\"inp\"]\n",
    "    assert stream.shape[0] == 1\n",
    "    stream = stream[0]\n",
    "\n",
    "    def play_stream(t, stream=stream):\n",
    "        ti = int(t / dt)\n",
    "        return stream[ti % len(stream)]\n",
    "\n",
    "    model.inp.output = play_stream\n",
    "    n_steps = stream.shape[0]\n",
    "\n",
    "    sim = nengo_loihi.Simulator(model, dt=dt, precompute=True)\n",
    "    with sim:\n",
    "        sim.run_steps(n_steps)\n",
    "\n",
    "    p_text = predict_text(sim, n_steps, p_time=10)\n",
    "    print(\"Predicted:\\t%s\" % p_text)\n",
    "    print(\"Actual:\\t\\t%s\" % text)\n",
    "\n",
    "    if text == 'aloha':\n",
    "        stats[\"aloha\"] += 1\n",
    "        if p_text in allowed_text:\n",
    "            print(\"True positive\")\n",
    "            stats[\"tp\"] += 1\n",
    "        else:\n",
    "            print(\"False negative\")\n",
    "            stats[\"fn\"] += 1\n",
    "    else:\n",
    "        stats[\"not-aloha\"] += 1\n",
    "        if p_text in allowed_text:\n",
    "            print(\"False positive\")\n",
    "            stats[\"fp\"] += 1\n",
    "        else:\n",
    "            print(\"True negative\")\n",
    "            stats[\"tn\"] += 1\n",
    "    print(\"\")\n",
    "\n",
    "print(\"Summary\")\n",
    "print(\"=======\")\n",
    "print(\"True positive rate:\\t%.3f\" % (stats[\"tp\"] / stats[\"aloha\"]))\n",
    "print(\"False negative rate:\\t%.3f\" % (stats[\"fn\"] / stats[\"not-aloha\"]))\n",
    "print()\n",
    "print(\"True negative rate:\\t%.3f\" % (stats[\"tn\"] / stats[\"not-aloha\"]))\n",
    "print(\"False positive rate:\\t%.3f\" % (stats[\"fp\"] / stats[\"aloha\"]))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 3
}
