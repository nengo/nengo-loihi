{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DVS from file\n",
    "\n",
    "This example demonstrates how to load pre-recorded Dynamic Vision Sensor (DVS) event\n",
    "data from a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import HTML\n",
    "from matplotlib.animation import ArtistAnimation\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import nengo\n",
    "import nengo_loihi\n",
    "\n",
    "# All NengoLoihi models should call this before model construction\n",
    "nengo_loihi.set_defaults()\n",
    "\n",
    "rng = np.random.RandomState(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate synthetic data\n",
    "\n",
    "Rather than using real DVS data, we will generate some synthetic data and save it in a\n",
    "`.events` file. In most applications, this will not be necessary, since you will already\n",
    "have a `.events` or `.aedat` file from a real DVS camera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jitter_time(n, t, jitter, rng, dtype=\"<u4\"):\n",
    "    assert jitter >= 0\n",
    "    assert t - jitter >= 0\n",
    "    tt = (t - jitter) * np.ones(n, dtype=dtype)\n",
    "    if jitter > 0:\n",
    "        tt += rng.randint(0, 2 * jitter + 1, size=tt.shape, dtype=dtype)\n",
    "    return tt\n",
    "\n",
    "\n",
    "# the height and width of the DVS sensor\n",
    "dvs_height = 180\n",
    "dvs_width = 240\n",
    "\n",
    "# our timestep in microseconds (Î¼s)\n",
    "dt_us = 1000\n",
    "\n",
    "# the maximum amount by which to jitter spikes around the timestep (in microseconds)\n",
    "t_jitter_us = 100\n",
    "\n",
    "assert t_jitter_us < dt_us // 2\n",
    "\n",
    "# the length of time to generate data for, in seconds and in microseconds\n",
    "t_length = 1.0\n",
    "t_length_us = int(1e6 * t_length)\n",
    "\n",
    "# the maximum rate of input spikes (per pixel)\n",
    "max_rate = 10\n",
    "max_prob = max_rate * 1e-6 * dt_us\n",
    "\n",
    "# the period of the sine wave, in pixels\n",
    "period = 120\n",
    "\n",
    "# these functions control the angle (theta) and phase of the sine wave over time\n",
    "theta_fn = lambda t: 1\n",
    "phase_fn = lambda t: 10 * t\n",
    "\n",
    "X, Y = np.meshgrid(np.linspace(-1, 1, dvs_width), np.linspace(-1, 1, dvs_height))\n",
    "\n",
    "events = []\n",
    "for t_us in range(dt_us, t_length_us + 1, dt_us):\n",
    "    t = t_us * 1e-6\n",
    "    theta = theta_fn(t)\n",
    "    phase = phase_fn(t)\n",
    "\n",
    "    X1 = np.cos(theta) * X + np.sin(theta) * Y\n",
    "\n",
    "    x = np.linspace(-1.5, 1.5, 50)\n",
    "    prob = np.sin((np.pi * dvs_height / period) * x + phase) * max_prob\n",
    "    prob = np.interp(X1, x, prob)\n",
    "\n",
    "    u = rng.rand(*prob.shape)\n",
    "    s_on = u < prob\n",
    "    s_off = u < -prob\n",
    "\n",
    "    y, x = s_off.nonzero()\n",
    "    tt = jitter_time(len(x), t_us, t_jitter_us, rng, dtype=\"<u4\")\n",
    "    events.append((tt, 0, x, y))\n",
    "\n",
    "    y, x = s_on.nonzero()\n",
    "    tt = jitter_time(len(x), t_us, t_jitter_us, rng, dtype=\"<u4\")\n",
    "    events.append((tt, 1, x, y))\n",
    "\n",
    "dvs_events = nengo_loihi.dvs.DVSEvents()\n",
    "dvs_events.init_events(n_events=sum(len(xx) for _, _, xx, _ in events))\n",
    "\n",
    "i = 0\n",
    "for tt, p, xx, yy in events:\n",
    "    ee = dvs_events.events[i : i + len(xx)]\n",
    "    ee[\"t\"] = tt\n",
    "    ee[\"p\"] = p\n",
    "    ee[\"x\"] = xx\n",
    "    ee[\"y\"] = yy\n",
    "    i += len(xx)\n",
    "\n",
    "events_file_name = \"dvs-from-file-events.events\"\n",
    "dvs_events.write_file(events_file_name)\n",
    "print(\"Wrote %r\" % events_file_name)\n",
    "del dvs_events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view the data by using the `DVSEvents` class to load the events, group the events\n",
    "into frames, and then make the frames into a video with the help of Matplotlib's\n",
    "animation support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dvs_events = nengo_loihi.dvs.DVSEvents.from_file(events_file_name)\n",
    "\n",
    "dt_frame_us = 20e3\n",
    "t_frames = dt_frame_us * np.arange(int(round(t_length_us / dt_frame_us)))\n",
    "\n",
    "fig = plt.figure()\n",
    "imgs = []\n",
    "for t_frame in t_frames:\n",
    "    t0_us = t_frame\n",
    "    t1_us = t_frame + dt_frame_us\n",
    "    t = dvs_events.events[:][\"t\"]\n",
    "    m = (t >= t0_us) & (t < t1_us)\n",
    "    events_m = dvs_events.events[m]\n",
    "\n",
    "    # show \"off\" (0) events as -1 and \"on\" (1) events as +1\n",
    "    events_sign = 2.0 * events_m[\"p\"] - 1\n",
    "\n",
    "    frame_img = np.zeros((dvs_height, dvs_width))\n",
    "    frame_img[events_m[\"y\"], events_m[\"x\"]] = events_sign\n",
    "\n",
    "    img = plt.imshow(frame_img, vmin=-1, vmax=1, animated=True)\n",
    "    imgs.append([img])\n",
    "\n",
    "del dvs_events\n",
    "\n",
    "ani = ArtistAnimation(fig, imgs, interval=50, blit=True)\n",
    "HTML(ani.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nengo network\n",
    "\n",
    "We can now load our data into a NengoLoihi network, using the\n",
    "`nengo_loihi.dvs.DVSFileChipProcess` type of `nengo.Process`.\n",
    "When run in a `nengo_loihi.Simulator`,\n",
    "this process passes the DVS events read from the file\n",
    "directly to the Loihi board.\n",
    "It can also be used in a `nengo.Simulator`,\n",
    "in case you want to prototype your DVS networks in core Nengo first.\n",
    "\n",
    "We pass the process the path to our data file.\n",
    "We also use the optional `pool` parameter to pool over space.\n",
    "This allows our node to output at a lower spatial resolution\n",
    "than the standard 180 x 240 pixel DVS output.\n",
    "This can allow for a smaller network for processing the DVS data, and thus\n",
    "conserve Loihi resources. Note that when we connect the pooled input to the ensembles,\n",
    "we scale by an input factor of ``1 / np.prod(pool)``; the result is that our pooling\n",
    "averages over the input events, rather than simply summing them. This is not necessary,\n",
    "but can make it easier to keep network parameters within typical ranges (akin to\n",
    "normalizing ``Node`` outputs so that they are in the range ``[-1, 1]``, though in\n",
    "the case of DVS events, having more than one event per pixel within a simulator timestep\n",
    "can still result in values outside this range).\n",
    "\n",
    "We then make two ensembles: one to receive the positive polarity \"on\" events,\n",
    "and the other to receive the negative polarity \"off\" events. Since we passed\n",
    "`channels_last=True` to our node, the polarity will be the least-significant index of\n",
    "our node output, and we connect it to the ensembles accordingly (by sending all even\n",
    "outputs to the negative polarity ensemble `ensembles[0]`, and the odd outputs to the\n",
    "positive polarity ensemble `ensembles[1]`). We run the simulation and collect the spikes\n",
    "from both the negative and positive polarity ensembles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = (10, 10)\n",
    "\n",
    "gain = 101\n",
    "\n",
    "with nengo.Network() as net:\n",
    "    dvs_process = nengo_loihi.dvs.DVSFileChipProcess(\n",
    "        file_path=events_file_name, pool=pool, channels_last=True\n",
    "    )\n",
    "    u = nengo.Node(dvs_process)\n",
    "\n",
    "    ensembles = [\n",
    "        nengo.Ensemble(\n",
    "            dvs_process.height * dvs_process.width,\n",
    "            1,\n",
    "            neuron_type=nengo.SpikingRectifiedLinear(),\n",
    "            gain=nengo.dists.Choice([gain]),\n",
    "            bias=nengo.dists.Choice([0]),\n",
    "        )\n",
    "        for _ in range(dvs_process.polarity)\n",
    "    ]\n",
    "\n",
    "    for k, e in enumerate(ensembles):\n",
    "        u_channel = u[k :: dvs_process.polarity]\n",
    "        nengo.Connection(u_channel, e.neurons, transform=1.0 / np.prod(pool))\n",
    "\n",
    "    probes = [nengo.Probe(e.neurons) for e in ensembles]\n",
    "\n",
    "with nengo_loihi.Simulator(net) as sim:\n",
    "    sim.run(t_length)\n",
    "\n",
    "sim_t = sim.trange()\n",
    "shape = (len(sim_t), dvs_process.height, dvs_process.width)\n",
    "output_spikes_neg = sim.data[probes[0]].reshape(shape) * sim.dt\n",
    "output_spikes_pos = sim.data[probes[1]].reshape(shape) * sim.dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot individual frames by collecting spikes near particular points in time.\n",
    "However, it is much easier to see how the output changes over time when we plot it as a\n",
    "video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_frame = 0.01\n",
    "t_frames = dt_frame * np.arange(int(round(t_length / dt_frame)))\n",
    "\n",
    "fig = plt.figure()\n",
    "imgs = []\n",
    "for t_frame in t_frames:\n",
    "    t0 = t_frame\n",
    "    t1 = t_frame + dt_frame\n",
    "    m = (sim_t >= t0) & (sim_t < t1)\n",
    "\n",
    "    frame_img = np.zeros((dvs_process.height, dvs_process.width))\n",
    "    frame_img -= output_spikes_neg[m].sum(axis=0)\n",
    "    frame_img += output_spikes_pos[m].sum(axis=0)\n",
    "    frame_img = frame_img / np.abs(frame_img).max()\n",
    "\n",
    "    img = plt.imshow(frame_img, vmin=-1, vmax=1, animated=True)\n",
    "    imgs.append([img])\n",
    "\n",
    "ani = ArtistAnimation(fig, imgs, interval=50, blit=True)\n",
    "HTML(ani.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the output of the NengoLoihi network is qualitatively similar to the input\n",
    "DVS events we plotted above. This is what we would expect, since the network is not\n",
    "performing any significant computation, but simply relaying its input to its output. The\n",
    "key difference is that the resolution of the network output is much lower; this is\n",
    "because we perform pooling on the DVS data when inputting it to the network, so that the\n",
    "network is processing everything at the reduced resolution."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
